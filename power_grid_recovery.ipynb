{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-30T22:26:46.565824Z",
     "start_time": "2024-05-30T22:26:46.110567Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import odeint\n",
    "import pandas as pd\n",
    "import warnings\n",
    "pd.set_option('display.float_format', '{:0.8f}'.format)\n",
    "import operator\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import solve_ivp\n",
    "from scipy.sparse import coo_array"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T22:26:55.385600Z",
     "start_time": "2024-05-30T22:26:55.379787Z"
    }
   },
   "cell_type": "code",
   "source": "gamma_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_gamma.csv\")",
   "id": "cc0466f404ecfacc",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "7b7c8e65d35ed08d",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "38182cf85bf7d295",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "2fdaa11d34bbe2d",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "   2: \n",
    "   3: gamma_df\n",
    "   4: data_matrix_df_orig = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_timeseries.csv\")\n",
    "   5:\n",
    "skip_n_rows_btw = 100\n",
    "rows_to_keep = np.arange(0, len(data_matrix_df_orig), skip_n_rows_btw)\n",
    "   6: data_matrix_df = data_matrix_df_orig.iloc[rows_to_keep].reset_index(drop=True)\n",
    "   7:\n",
    "new_column_names = [\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
    "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\",\n",
    "                   \"Q_0\", \"Q_1\", \"Q_2\", \"Q_3\", \"Q_4\", \"Q_5\"]\n",
    "data_matrix_df.rename(columns=dict((zip(data_matrix_df.columns, new_column_names))),\n",
    "                     inplace=True)\n",
    "   8:\n",
    "data_matrix_df = data_matrix_df[[\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
    "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]]\n",
    "\n",
    "data_matrix_df\n",
    "   9: data_matrix_df.columns\n",
    "  10:\n",
    "admittance_Y_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_Y.csv\")\n",
    "for column in admittance_Y_df.columns:\n",
    "    admittance_Y_df[column] = admittance_Y_df[column].apply(lambda x: x.replace('i', 'j'))\n",
    "  11: admittance_Y_df\n",
    "  12: static_param_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_staticparams.csv\")\n",
    "  13: static_param_df\n",
    "  14: coupling_K_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_K.csv\")\n",
    "  15:\n",
    "coupling_K_df_labeled = coupling_K_df.set_index(coupling_K_df.columns)\n",
    "coupling_K_df_labeled\n",
    "  16: gamma_df\n",
    "  17:\n",
    "gamma_matrix = gamma_df.to_numpy()\n",
    "admittance_Y_matrix = admittance_Y_df.to_numpy()\n",
    "\n",
    "gamma_matrix\n",
    "  18:\n",
    "coupling_matrix_init = np.ones(admittance_Y_matrix.shape)\n",
    "# coupling_matrix_init = np.zeros(admittance_Y_matrix.shape)\n",
    "# coupling_matrix_init[3,:] = 1\n",
    "\n",
    "coupling_matrix_init = np.triu(coupling_matrix_init, 0)\n",
    "coupling_matrix_init\n",
    "sparse_coupling_matrix_init = coo_array(coupling_matrix_init)\n",
    "sparse_coupling_matrix_init.toarray()\n",
    "  19:\n",
    "from dae_finder import FeatureCouplingTransformer\n",
    "\n",
    "def coup_fun(x,y,i,j,gam_matrix):\n",
    "    # return np.sin(x-y)\n",
    "    return np.sin(x-y- gam_matrix[i,j])\n",
    "\n",
    "def coup_namer(x,y,i,j,gam_matrix):\n",
    "    return \"sin( {}-{} -gamma_{},{} )\".format(x,y,i,j)\n",
    "    \n",
    "\n",
    "dummy_tr_sin_diff = FeatureCouplingTransformer(sparse_coupling_matrix_init,\n",
    "                                           coupling_func= coup_fun,\n",
    "                                           coupling_namer= coup_namer,\n",
    "                                           coupling_func_args={\"gam_matrix\":gamma_matrix},\n",
    "                                              return_df=True)\n",
    "  20:\n",
    "sin_diff_library = dummy_tr_sin_diff.fit_transform(data_matrix_df.drop([\"time\"], axis=1))\n",
    "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
    "\n",
    "# cop_ind\n",
    "  21: cop_ind\n",
    "  22: sin_diff_library\n",
    "  23:\n",
    "candidate_lib = pd.concat([data_matrix_df.drop(\"time\", axis=1),\n",
    "                          sin_diff_library], axis=1)\n",
    "  24:\n",
    "non_zero_column_series = (candidate_lib**2).sum() > 0.00001\n",
    "non_zero_column_series\n",
    "non_columns = [column for column in candidate_lib if non_zero_column_series[column]]\n",
    "\n",
    "candidate_lib = candidate_lib[non_columns]\n",
    "  25: candidate_lib\n",
    "  26:\n",
    "from dae_finder import add_noise_to_df\n",
    "noise_perc = 0\n",
    "data_matrix_df_list = [data_matrix_df]\n",
    "num_time_points = len(data_matrix_df)\n",
    "data_matrix_features = data_matrix_df_list[0].columns\n",
    "for ind, data_matrix_ in enumerate(data_matrix_df_list):\n",
    "    t_exact = data_matrix_[\"time\"]\n",
    "    noisy_data_df = add_noise_to_df(data_matrix_, noise_perc=noise_perc, random_seed=111)\n",
    "    noisy_data_df[\"time\"] = t_exact\n",
    "    data_matrix_df_list[ind] = noisy_data_df\n",
    "  27:\n",
    "from dae_finder import smooth_data\n",
    "\n",
    "#Calling the smoothening function\n",
    "data_matrix_smooth_df_list = [smooth_data(data_matrix,domain_var=\"time\",derr_order=1, noise_perc=noise_perc) for data_matrix in data_matrix_df_list]\n",
    "\n",
    "if len(data_matrix_df_list) >1:\n",
    "    data_matrix_df_smooth_appended = pd.concat(data_matrix_smooth_df_list, ignore_index=True)\n",
    "else:\n",
    "    data_matrix_df_smooth_appended = data_matrix_smooth_df_list[0]\n",
    "\n",
    "data_matrix_df_smooth = data_matrix_df_smooth_appended[data_matrix_features]\n",
    "# if \"time\" in data_matrix_df_smooth:\n",
    "#     data_matrix_df_smooth = data_matrix_df_smooth.drop(\"time\", axis=1)\n",
    "  28: data_matrix_df_smooth- data_matrix_df\n",
    "  29:\n",
    "\n",
    "ind = 0\n",
    "feature_ = \"Phi_5\"\n",
    "\n",
    "plt.figure()\n",
    "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
    "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
    "\n",
    "plt.plot(data_matrix_df_list[ind][\"time\"], data_matrix_df_list[ind][feature_], \".\", data_matrix_smooth_df_list[ind][\"time\"],\n",
    "         data_matrix_smooth_df_list[ind][feature_],\".\",data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"time\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \".\")\n",
    "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
    "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
    "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
    "plt.show()\n",
    "  30:\n",
    "# Removing some of the outliers coming from sudden jump during perturbations\n",
    "\n",
    "new_df = data_matrix_df_smooth_appended[abs(data_matrix_df_smooth_appended) <= 20]\n",
    "\n",
    "\n",
    "plt.plot(new_df[[\"time\"]], new_df[[\"d(Phi_0) /dt\"]], \".\",\n",
    "         new_df[[\"time\"]], new_df[[\"om_0\"]], \".\",\n",
    "        new_df[[\"time\"]], new_df[[\"d(om_0) /dt\"]], \".\")\n",
    "\n",
    "new_df.plot()\n",
    "  31:\n",
    "import sympy\n",
    "\n",
    "from dae_finder import get_refined_lib, remove_paranth_from_feat\n",
    "\n",
    "# Adding the state variables as scipy symbols\n",
    "feat_list = list(data_matrix_df.columns)\n",
    "feat_list_str = \", \".join(remove_paranth_from_feat(data_matrix_df.columns))\n",
    "exec(feat_list_str+ \"= sympy.symbols(\"+str(feat_list)+\")\")\n",
    "  32:\n",
    "from dae_finder import sequentialThLin, AlgModelFinder\n",
    "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
    "                                       alpha=0.3,\n",
    "                                       fit_intercept=True)\n",
    "  33:\n",
    "features_to_fit_ = [\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
    "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
    "# features_to_fit_ = [\"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
    "num_nodes = 6\n",
    "power_features = [\"P_{}\".format(ind) for ind in range(num_nodes)]\n",
    "#Mapping each power feature to possible expressions in the algebraic relationship\n",
    "feature_to_libr_map = {power_feat: candidate_lib.columns.drop(power_features) for power_feat in power_features}\n",
    "\n",
    "\n",
    "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
    "                          features_to_fit=features_to_fit_,\n",
    "                         feature_to_library_map=feature_to_libr_map)\n",
    "  34: algebraic_model_lasso.best_models()\n",
    "  35: algebraic_model_lasso.best_models()[\"P_0\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.01]\n",
    "  36:\n",
    "feat = \"P_2\"\n",
    "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
    "  37:\n",
    "features_to_fit_ = [\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
    "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
    "# features_to_fit_ = [\"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
    "num_nodes = 6\n",
    "power_features = [\"P_{}\".format(ind) for ind in range(num_nodes)]\n",
    "#Mapping each power feature to possible expressions in the algebraic relationship\n",
    "feature_to_libr_map = {power_feat: candidate_lib.columns.drop(power_features) for power_feat in power_features}\n",
    "\n",
    "\n",
    "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
    "                          features_to_fit=features_to_fit_,\n",
    "                         feature_to_library_map=feature_to_libr_map)\n",
    "  38: algebraic_model_lasso.best_models()\n",
    "  39: algebraic_model_lasso.best_models()[\"P_0\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.01]\n",
    "  40:\n",
    "feat = \"P_2\"\n",
    "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
    "  41:\n",
    "# from dae_finder import sequentialThLin\n",
    "\n",
    "# seq_th_model = sequentialThLin(fit_intercept=False)\n",
    "\n",
    "# seq_th_model.fit(X=candidate_lib_full,  y=data_matrix_df_smooth_appended['d([P]) /dt'])\n",
    "# features_to_remove = {E, S*ES}\n",
    "\n",
    "# features_to_remove, refined_candid_lib = get_refined_lib(features_to_remove, data_matrix_df,\n",
    "#                                                   candidate_lib_full, get_dropped_feat=True)\n",
    "\n",
    "# refined_candid_lib = candidate_lib[['Phi_0', 'Phi_1', 'Phi_2', 'Phi_3', 'Phi_4', 'Phi_5', 'om_0',\n",
    "#        'om_1', 'P_0', 'P_1', 'P_2', 'P_3', 'P_4', 'P_5']]\n",
    "refined_candid_lib = data_matrix_df_smooth_appended[['Phi_0', 'Phi_1', 'Phi_2', 'Phi_3', 'Phi_4', 'Phi_5', 'om_0',\n",
    "       'om_1', 'P_0', 'P_1', 'P_2', 'P_3', 'P_4', 'P_5']]\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "s_scaler = StandardScaler(with_std=True, with_mean=False)\n",
    "scaled_refined_lib = pd.DataFrame(s_scaler.fit_transform(refined_candid_lib), columns=s_scaler.feature_names_in_)\n",
    "scaled_cand_lib = pd.DataFrame(s_scaler.fit_transform(candidate_lib), columns=s_scaler.feature_names_in_)\n",
    "  42:\n",
    "from sklearn.linear_model import Lasso\n",
    "alg_lasso = Lasso(fit_intercept=True, alpha=0.3)\n",
    "alg_lasso.fit(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
    "alg_lasso.score(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
    "  43: dict(zip(alg_lasso.feature_names_in_, alg_lasso.coef_))\n",
    "  44:\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_model = LinearRegression()\n",
    "lin_model.fit(X=scaled_refined_lib[[\"[ES]\"]],  y=data_matrix_df_smooth_appended['d([P]) /dt'])\n",
    "  45: lin_model.intercept_\n",
    "  46: alg_lasso.fit(X=scaled_cand_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
    "  47: dict(zip(alg_lasso.feature_names_in_, alg_lasso.coef_))\n",
    "  48:\n",
    "from dae_finder import sequentialThLin, AlgModelFinder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# lin_reg_model = LinearRegression\n",
    "# lin_reg_model_arg = {\"fit_intercept\": True}\n",
    "# seq_th_model = sequentialThLin(custom_model=True,\n",
    "#                                custom_model_ob = lin_reg_model,\n",
    "#                                custom_model_arg= lin_reg_model_arg,\n",
    "#                               coef_threshold=0.1)\n",
    "seq_th_model = sequentialThLin(coef_threshold=0.1, fit_intercept=True)\n",
    "\n",
    "algebraic_model_th = AlgModelFinder(custom_model=True, custom_model_ob= seq_th_model)\n",
    "  49:\n",
    "algebraic_model_th.fit(candidate_lib, scale_columns= True,\n",
    "                          features_to_fit=features_to_fit_,\n",
    "                         feature_to_library_map=feature_to_libr_map)\n",
    "  50:\n",
    "#Best 10 models using R2 metrix\n",
    "algebraic_model_th.best_models()\n",
    "  51: algebraic_model_th.get_fitted_intercepts()\n",
    "  52:\n",
    "feat = \"P_3\"\n",
    "algebraic_model_th.best_models()[feat][abs(algebraic_model_th.best_models()[feat])>0.1]\n",
    "  53:\n",
    "feat = \"P_2\"\n",
    "algebraic_model_th.best_models()[feat][abs(algebraic_model_th.best_models()[feat])>0.1]\n",
    "  54:\n",
    "from dae_finder import sequentialThLin, AlgModelFinder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# lin_reg_model = LinearRegression\n",
    "# lin_reg_model_arg = {\"fit_intercept\": True}\n",
    "# seq_th_model = sequentialThLin(custom_model=True,\n",
    "#                                custom_model_ob = lin_reg_model,\n",
    "#                                custom_model_arg= lin_reg_model_arg,\n",
    "#                               coef_threshold=0.1)\n",
    "seq_th_model = sequentialThLin(model_id=\"lasso\",coef_threshold=0.1, fit_intercept=True)\n",
    "\n",
    "seq_th_model.fit(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_1) /dt'])\n",
    "seq_th_model.score(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_1) /dt'])\n",
    "  55: dict(zip(seq_th_model.feature_names_in_, seq_th_model.coef_))\n",
    "  56:\n",
    "from dae_finder import sequentialThLin, AlgModelFinder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# lin_reg_model = LinearRegression\n",
    "# lin_reg_model_arg = {\"fit_intercept\": True}\n",
    "# seq_th_model = sequentialThLin(custom_model=True,\n",
    "#                                custom_model_ob = lin_reg_model,\n",
    "#                                custom_model_arg= lin_reg_model_arg,\n",
    "#                               coef_threshold=0.1)\n",
    "seq_th_model = sequentialThLin(model_id=\"lasso\",coef_threshold=0.1, fit_intercept=True)\n",
    "\n",
    "seq_th_model.fit(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
    "seq_th_model.score(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
    "  57: dict(zip(seq_th_model.feature_names_in_, seq_th_model.coef_))\n",
    "  58:\n",
    "from dae_finder import sequentialThLin, AlgModelFinder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# lin_reg_model = LinearRegression\n",
    "# lin_reg_model_arg = {\"fit_intercept\": True}\n",
    "# seq_th_model = sequentialThLin(custom_model=True,\n",
    "#                                custom_model_ob = lin_reg_model,\n",
    "#                                custom_model_arg= lin_reg_model_arg,\n",
    "#                               coef_threshold=0.1)\n",
    "seq_th_model = sequentialThLin(model_id=\"lasso\",coef_threshold=0.5, fit_intercept=True)\n",
    "\n",
    "seq_th_model.fit(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
    "seq_th_model.score(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
    "  59: dict(zip(seq_th_model.feature_names_in_, seq_th_model.coef_))\n",
    "  60:\n",
    "from dae_finder import sequentialThLin, AlgModelFinder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# lin_reg_model = LinearRegression\n",
    "# lin_reg_model_arg = {\"fit_intercept\": True}\n",
    "# seq_th_model = sequentialThLin(custom_model=True,\n",
    "#                                custom_model_ob = lin_reg_model,\n",
    "#                                custom_model_arg= lin_reg_model_arg,\n",
    "#                               coef_threshold=0.1)\n",
    "seq_th_model = sequentialThLin(model_id=\"lasso\",coef_threshold=0.1, fit_intercept=True)\n",
    "\n",
    "seq_th_model.fit(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
    "seq_th_model.score(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
    "  61: dict(zip(seq_th_model.feature_names_in_, seq_th_model.coef_))\n",
    "  62: seq_th_model.intercept_\n",
    "  63:\n",
    "coef_dict = dict(zip(seq_th_model.feature_names_in_, seq_th_model.coef_))\n",
    "coef_dict\n",
    "  64: non_zero_feat = [feat for feat,coef in coef_dict.items() if abs(coef)>0.01]\n",
    "  65: non_zero_feat\n",
    "  66:\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_model = LinearRegression(fit_intercept=True)\n",
    "lin_model.fit(X=X=scaled_refined_lib[non_zero_feat],  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
    "  67:\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_model = LinearRegression(fit_intercept=True)\n",
    "lin_model.fit(X=scaled_refined_lib[non_zero_feat],  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
    "  68:\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_model = LinearRegression(fit_intercept=True)\n",
    "lin_model.fit(X=scaled_refined_lib[non_zero_feat],  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
    "lin_model.score(X=scaled_refined_lib[non_zero_feat],  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
    "  69: dict(zip(lin_model.feature_names_in_, lin_model.coef_))\n",
    "  70:\n",
    "seq_th_model.fit(X=scaled_cand_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
    "seq_th_model.score(X=scaled_cand_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
    "  71: dict(zip(seq_th_model.feature_names_in_, seq_th_model.coef_))\n",
    "  72:\n",
    "skip_n_rows_btw = 10\n",
    "rows_to_keep = np.arange(0, len(data_matrix_df_orig), skip_n_rows_btw)\n",
    "  73: data_matrix_df = data_matrix_df_orig.iloc[rows_to_keep].reset_index(drop=True)\n",
    "  74:\n",
    "new_column_names = [\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
    "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\",\n",
    "                   \"Q_0\", \"Q_1\", \"Q_2\", \"Q_3\", \"Q_4\", \"Q_5\"]\n",
    "data_matrix_df.rename(columns=dict((zip(data_matrix_df.columns, new_column_names))),\n",
    "                     inplace=True)\n",
    "  75:\n",
    "data_matrix_df = data_matrix_df[[\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
    "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]]\n",
    "\n",
    "data_matrix_df\n",
    "  76: data_matrix_df.columns\n",
    "  77:\n",
    "admittance_Y_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_Y.csv\")\n",
    "for column in admittance_Y_df.columns:\n",
    "    admittance_Y_df[column] = admittance_Y_df[column].apply(lambda x: x.replace('i', 'j'))\n",
    "  78: admittance_Y_df\n",
    "  79: static_param_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_staticparams.csv\")\n",
    "  80: static_param_df\n",
    "  81: coupling_K_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_K.csv\")\n",
    "  82:\n",
    "coupling_K_df_labeled = coupling_K_df.set_index(coupling_K_df.columns)\n",
    "coupling_K_df_labeled\n",
    "  83: gamma_df\n",
    "  84:\n",
    "gamma_matrix = gamma_df.to_numpy()\n",
    "admittance_Y_matrix = admittance_Y_df.to_numpy()\n",
    "\n",
    "gamma_matrix\n",
    "  85:\n",
    "coupling_matrix_init = np.ones(admittance_Y_matrix.shape)\n",
    "# coupling_matrix_init = np.zeros(admittance_Y_matrix.shape)\n",
    "# coupling_matrix_init[3,:] = 1\n",
    "\n",
    "coupling_matrix_init = np.triu(coupling_matrix_init, 0)\n",
    "coupling_matrix_init\n",
    "sparse_coupling_matrix_init = coo_array(coupling_matrix_init)\n",
    "sparse_coupling_matrix_init.toarray()\n",
    "  86:\n",
    "from dae_finder import FeatureCouplingTransformer\n",
    "\n",
    "def coup_fun(x,y,i,j,gam_matrix):\n",
    "    # return np.sin(x-y)\n",
    "    return np.sin(x-y- gam_matrix[i,j])\n",
    "\n",
    "def coup_namer(x,y,i,j,gam_matrix):\n",
    "    return \"sin( {}-{} -gamma_{},{} )\".format(x,y,i,j)\n",
    "    \n",
    "\n",
    "dummy_tr_sin_diff = FeatureCouplingTransformer(sparse_coupling_matrix_init,\n",
    "                                           coupling_func= coup_fun,\n",
    "                                           coupling_namer= coup_namer,\n",
    "                                           coupling_func_args={\"gam_matrix\":gamma_matrix},\n",
    "                                              return_df=True)\n",
    "  87:\n",
    "sin_diff_library = dummy_tr_sin_diff.fit_transform(data_matrix_df.drop([\"time\"], axis=1))\n",
    "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
    "\n",
    "# cop_ind\n",
    "  88: cop_ind\n",
    "  89: sin_diff_library\n",
    "  90:\n",
    "candidate_lib = pd.concat([data_matrix_df.drop(\"time\", axis=1),\n",
    "                          sin_diff_library], axis=1)\n",
    "  91:\n",
    "non_zero_column_series = (candidate_lib**2).sum() > 0.00001\n",
    "non_zero_column_series\n",
    "non_columns = [column for column in candidate_lib if non_zero_column_series[column]]\n",
    "\n",
    "candidate_lib = candidate_lib[non_columns]\n",
    "  92: candidate_lib\n",
    "  93:\n",
    "from dae_finder import add_noise_to_df\n",
    "noise_perc = 0\n",
    "data_matrix_df_list = [data_matrix_df]\n",
    "num_time_points = len(data_matrix_df)\n",
    "data_matrix_features = data_matrix_df_list[0].columns\n",
    "for ind, data_matrix_ in enumerate(data_matrix_df_list):\n",
    "    t_exact = data_matrix_[\"time\"]\n",
    "    noisy_data_df = add_noise_to_df(data_matrix_, noise_perc=noise_perc, random_seed=111)\n",
    "    noisy_data_df[\"time\"] = t_exact\n",
    "    data_matrix_df_list[ind] = noisy_data_df\n",
    "  94:\n",
    "from dae_finder import smooth_data\n",
    "\n",
    "#Calling the smoothening function\n",
    "data_matrix_smooth_df_list = [smooth_data(data_matrix,domain_var=\"time\",derr_order=1, noise_perc=noise_perc) for data_matrix in data_matrix_df_list]\n",
    "\n",
    "if len(data_matrix_df_list) >1:\n",
    "    data_matrix_df_smooth_appended = pd.concat(data_matrix_smooth_df_list, ignore_index=True)\n",
    "else:\n",
    "    data_matrix_df_smooth_appended = data_matrix_smooth_df_list[0]\n",
    "\n",
    "data_matrix_df_smooth = data_matrix_df_smooth_appended[data_matrix_features]\n",
    "# if \"time\" in data_matrix_df_smooth:\n",
    "#     data_matrix_df_smooth = data_matrix_df_smooth.drop(\"time\", axis=1)\n",
    "  95: data_matrix_df_smooth- data_matrix_df\n",
    "  96:\n",
    "\n",
    "ind = 0\n",
    "feature_ = \"Phi_5\"\n",
    "\n",
    "plt.figure()\n",
    "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
    "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
    "\n",
    "plt.plot(data_matrix_df_list[ind][\"time\"], data_matrix_df_list[ind][feature_], \".\", data_matrix_smooth_df_list[ind][\"time\"],\n",
    "         data_matrix_smooth_df_list[ind][feature_],\".\",data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"time\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \".\")\n",
    "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
    "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
    "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
    "plt.show()\n",
    "  97:\n",
    "# Removing some of the outliers coming from sudden jump during perturbations\n",
    "\n",
    "new_df = data_matrix_df_smooth_appended[abs(data_matrix_df_smooth_appended) <= 20]\n",
    "\n",
    "\n",
    "plt.plot(new_df[[\"time\"]], new_df[[\"d(Phi_0) /dt\"]], \".\",\n",
    "         new_df[[\"time\"]], new_df[[\"om_0\"]], \".\",\n",
    "        new_df[[\"time\"]], new_df[[\"d(om_0) /dt\"]], \".\")\n",
    "\n",
    "new_df.plot()\n",
    "  98:\n",
    "import sympy\n",
    "\n",
    "from dae_finder import get_refined_lib, remove_paranth_from_feat\n",
    "\n",
    "# Adding the state variables as scipy symbols\n",
    "feat_list = list(data_matrix_df.columns)\n",
    "feat_list_str = \", \".join(remove_paranth_from_feat(data_matrix_df.columns))\n",
    "exec(feat_list_str+ \"= sympy.symbols(\"+str(feat_list)+\")\")\n",
    "  99:\n",
    "from dae_finder import sequentialThLin, AlgModelFinder\n",
    "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
    "                                       alpha=0.3,\n",
    "                                       fit_intercept=True)\n",
    " 100:\n",
    "features_to_fit_ = [\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
    "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
    "# features_to_fit_ = [\"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
    "num_nodes = 6\n",
    "power_features = [\"P_{}\".format(ind) for ind in range(num_nodes)]\n",
    "#Mapping each power feature to possible expressions in the algebraic relationship\n",
    "feature_to_libr_map = {power_feat: candidate_lib.columns.drop(power_features) for power_feat in power_features}\n",
    "\n",
    "\n",
    "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
    "                          features_to_fit=features_to_fit_,\n",
    "                         feature_to_library_map=feature_to_libr_map)\n",
    " 101: algebraic_model_lasso.best_models()\n",
    " 102: algebraic_model_lasso.best_models()[\"P_0\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.01]\n",
    " 103:\n",
    "feat = \"P_2\"\n",
    "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
    " 104:\n",
    "feat = \"P_2\"\n",
    "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
    " 105:\n",
    "feat = \"P_2\"\n",
    "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
    " 106:\n",
    "feat = \"P_2\"\n",
    "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
    " 107:\n",
    "skip_n_rows_btw = 100\n",
    "rows_to_keep = np.arange(0, len(data_matrix_df_orig), skip_n_rows_btw)\n",
    " 108: data_matrix_df = data_matrix_df_orig.iloc[rows_to_keep].reset_index(drop=True)\n",
    " 109:\n",
    "new_column_names = [\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
    "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\",\n",
    "                   \"Q_0\", \"Q_1\", \"Q_2\", \"Q_3\", \"Q_4\", \"Q_5\"]\n",
    "data_matrix_df.rename(columns=dict((zip(data_matrix_df.columns, new_column_names))),\n",
    "                     inplace=True)\n",
    " 110:\n",
    "data_matrix_df = data_matrix_df[[\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
    "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]]\n",
    "\n",
    "data_matrix_df\n",
    " 111: data_matrix_df.columns\n",
    " 112:\n",
    "admittance_Y_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_Y.csv\")\n",
    "for column in admittance_Y_df.columns:\n",
    "    admittance_Y_df[column] = admittance_Y_df[column].apply(lambda x: x.replace('i', 'j'))\n",
    " 113: admittance_Y_df\n",
    " 114: static_param_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_staticparams.csv\")\n",
    " 115: static_param_df\n",
    " 116: coupling_K_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_K.csv\")\n",
    " 117:\n",
    "coupling_K_df_labeled = coupling_K_df.set_index(coupling_K_df.columns)\n",
    "coupling_K_df_labeled\n",
    " 118: gamma_df\n",
    " 119:\n",
    "gamma_matrix = gamma_df.to_numpy()\n",
    "admittance_Y_matrix = admittance_Y_df.to_numpy()\n",
    "\n",
    "gamma_matrix\n",
    " 120:\n",
    "coupling_matrix_init = np.ones(admittance_Y_matrix.shape)\n",
    "# coupling_matrix_init = np.zeros(admittance_Y_matrix.shape)\n",
    "# coupling_matrix_init[3,:] = 1\n",
    "\n",
    "coupling_matrix_init = np.triu(coupling_matrix_init, 0)\n",
    "coupling_matrix_init\n",
    "sparse_coupling_matrix_init = coo_array(coupling_matrix_init)\n",
    "sparse_coupling_matrix_init.toarray()\n",
    " 121:\n",
    "from dae_finder import FeatureCouplingTransformer\n",
    "\n",
    "def coup_fun(x,y,i,j,gam_matrix):\n",
    "    # return np.sin(x-y)\n",
    "    return np.sin(x-y- gam_matrix[i,j])\n",
    "\n",
    "def coup_namer(x,y,i,j,gam_matrix):\n",
    "    return \"sin( {}-{} -gamma_{},{} )\".format(x,y,i,j)\n",
    "    \n",
    "\n",
    "dummy_tr_sin_diff = FeatureCouplingTransformer(sparse_coupling_matrix_init,\n",
    "                                           coupling_func= coup_fun,\n",
    "                                           coupling_namer= coup_namer,\n",
    "                                           coupling_func_args={\"gam_matrix\":gamma_matrix},\n",
    "                                              return_df=True)\n",
    " 122:\n",
    "sin_diff_library = dummy_tr_sin_diff.fit_transform(data_matrix_df.drop([\"time\"], axis=1))\n",
    "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
    "\n",
    "# cop_ind\n",
    " 123: cop_ind\n",
    " 124: sin_diff_library\n",
    " 125:\n",
    "candidate_lib = pd.concat([data_matrix_df.drop(\"time\", axis=1),\n",
    "                          sin_diff_library], axis=1)\n",
    " 126:\n",
    "non_zero_column_series = (candidate_lib**2).sum() > 0.00001\n",
    "non_zero_column_series\n",
    "non_columns = [column for column in candidate_lib if non_zero_column_series[column]]\n",
    "\n",
    "candidate_lib = candidate_lib[non_columns]\n",
    " 127: candidate_lib\n",
    " 128:\n",
    "from dae_finder import add_noise_to_df\n",
    "noise_perc = 0\n",
    "data_matrix_df_list = [data_matrix_df]\n",
    "num_time_points = len(data_matrix_df)\n",
    "data_matrix_features = data_matrix_df_list[0].columns\n",
    "for ind, data_matrix_ in enumerate(data_matrix_df_list):\n",
    "    t_exact = data_matrix_[\"time\"]\n",
    "    noisy_data_df = add_noise_to_df(data_matrix_, noise_perc=noise_perc, random_seed=111)\n",
    "    noisy_data_df[\"time\"] = t_exact\n",
    "    data_matrix_df_list[ind] = noisy_data_df\n",
    " 129:\n",
    "from dae_finder import smooth_data\n",
    "\n",
    "#Calling the smoothening function\n",
    "data_matrix_smooth_df_list = [smooth_data(data_matrix,domain_var=\"time\",derr_order=1, noise_perc=noise_perc) for data_matrix in data_matrix_df_list]\n",
    "\n",
    "if len(data_matrix_df_list) >1:\n",
    "    data_matrix_df_smooth_appended = pd.concat(data_matrix_smooth_df_list, ignore_index=True)\n",
    "else:\n",
    "    data_matrix_df_smooth_appended = data_matrix_smooth_df_list[0]\n",
    "\n",
    "data_matrix_df_smooth = data_matrix_df_smooth_appended[data_matrix_features]\n",
    "# if \"time\" in data_matrix_df_smooth:\n",
    "#     data_matrix_df_smooth = data_matrix_df_smooth.drop(\"time\", axis=1)\n",
    " 130: data_matrix_df_smooth- data_matrix_df\n",
    " 131:\n",
    "\n",
    "ind = 0\n",
    "feature_ = \"Phi_5\"\n",
    "\n",
    "plt.figure()\n",
    "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
    "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
    "\n",
    "plt.plot(data_matrix_df_list[ind][\"time\"], data_matrix_df_list[ind][feature_], \".\", data_matrix_smooth_df_list[ind][\"time\"],\n",
    "         data_matrix_smooth_df_list[ind][feature_],\".\",data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"time\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \".\")\n",
    "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
    "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
    "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
    "plt.show()\n",
    " 132:\n",
    "# Removing some of the outliers coming from sudden jump during perturbations\n",
    "\n",
    "new_df = data_matrix_df_smooth_appended[abs(data_matrix_df_smooth_appended) <= 20]\n",
    "\n",
    "\n",
    "plt.plot(new_df[[\"time\"]], new_df[[\"d(Phi_0) /dt\"]], \".\",\n",
    "         new_df[[\"time\"]], new_df[[\"om_0\"]], \".\",\n",
    "        new_df[[\"time\"]], new_df[[\"d(om_0) /dt\"]], \".\")\n",
    "\n",
    "new_df.plot()\n",
    " 133:\n",
    "import sympy\n",
    "\n",
    "from dae_finder import get_refined_lib, remove_paranth_from_feat\n",
    "\n",
    "# Adding the state variables as scipy symbols\n",
    "feat_list = list(data_matrix_df.columns)\n",
    "feat_list_str = \", \".join(remove_paranth_from_feat(data_matrix_df.columns))\n",
    "exec(feat_list_str+ \"= sympy.symbols(\"+str(feat_list)+\")\")\n",
    " 134:\n",
    "from dae_finder import sequentialThLin, AlgModelFinder\n",
    "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
    "                                       alpha=0.3,\n",
    "                                       fit_intercept=True)\n",
    " 135:\n",
    "features_to_fit_ = [\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
    "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
    "# features_to_fit_ = [\"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
    "num_nodes = 6\n",
    "power_features = [\"P_{}\".format(ind) for ind in range(num_nodes)]\n",
    "#Mapping each power feature to possible expressions in the algebraic relationship\n",
    "feature_to_libr_map = {power_feat: candidate_lib.columns.drop(power_features) for power_feat in power_features}\n",
    "\n",
    "\n",
    "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
    "                          features_to_fit=features_to_fit_,\n",
    "                         feature_to_library_map=feature_to_libr_map)\n",
    " 136: algebraic_model_lasso.best_models()\n",
    " 137:\n",
    "feat = \"P_2\"\n",
    "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
    " 138:\n",
    "import numpy as np\n",
    "from scipy.integrate import odeint\n",
    "import pandas as pd\n",
    "import warnings\n",
    "pd.set_option('display.float_format', '{:0.8f}'.format)\n",
    "import operator\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import solve_ivp\n",
    "from scipy.sparse import coo_array\n",
    " 139: gamma_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_gamma.csv\")\n",
    " 140: gamma_df\n",
    " 141: data_matrix_df_orig = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_timeseries.csv\")\n",
    " 142:\n",
    "skip_n_rows_btw = 100\n",
    "rows_to_keep = np.arange(0, len(data_matrix_df_orig), skip_n_rows_btw)\n",
    " 143: data_matrix_df = data_matrix_df_orig.iloc[rows_to_keep].reset_index(drop=True)\n",
    " 144:\n",
    "new_column_names = [\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
    "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\",\n",
    "                   \"Q_0\", \"Q_1\", \"Q_2\", \"Q_3\", \"Q_4\", \"Q_5\"]\n",
    "data_matrix_df.rename(columns=dict((zip(data_matrix_df.columns, new_column_names))),\n",
    "                     inplace=True)\n",
    " 145:\n",
    "data_matrix_df = data_matrix_df[[\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
    "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]]\n",
    "\n",
    "data_matrix_df\n",
    " 146: data_matrix_df.columns\n",
    " 147:\n",
    "admittance_Y_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_Y.csv\")\n",
    "for column in admittance_Y_df.columns:\n",
    "    admittance_Y_df[column] = admittance_Y_df[column].apply(lambda x: x.replace('i', 'j'))\n",
    " 148: admittance_Y_df\n",
    " 149: static_param_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_staticparams.csv\")\n",
    " 150: static_param_df\n",
    " 151: coupling_K_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_K.csv\")\n",
    " 152:\n",
    "coupling_K_df_labeled = coupling_K_df.set_index(coupling_K_df.columns)\n",
    "coupling_K_df_labeled\n",
    " 153: gamma_df\n",
    " 154:\n",
    "gamma_matrix = gamma_df.to_numpy()\n",
    "admittance_Y_matrix = admittance_Y_df.to_numpy()\n",
    "\n",
    "gamma_matrix\n",
    " 155:\n",
    "coupling_matrix_init = np.ones(admittance_Y_matrix.shape)\n",
    "# coupling_matrix_init = np.zeros(admittance_Y_matrix.shape)\n",
    "# coupling_matrix_init[3,:] = 1\n",
    "\n",
    "coupling_matrix_init = np.triu(coupling_matrix_init, 0)\n",
    "coupling_matrix_init\n",
    "sparse_coupling_matrix_init = coo_array(coupling_matrix_init)\n",
    "sparse_coupling_matrix_init.toarray()\n",
    " 156:\n",
    "from dae_finder import FeatureCouplingTransformer\n",
    "\n",
    "def coup_fun(x,y,i,j,gam_matrix):\n",
    "    return np.sin(x-y- gam_matrix[i,j])\n",
    "\n",
    "def coup_namer(x,y,i,j,gam_matrix):\n",
    "    return \"sin( {}-{} -gamma_{},{} )\".format(x,y,i,j)\n",
    "    \n",
    "\n",
    "dummy_tr_sin_diff = FeatureCouplingTransformer(sparse_coupling_matrix_init,\n",
    "                                           coupling_func= coup_fun,\n",
    "                                           coupling_namer= coup_namer,\n",
    "                                           coupling_func_args={\"gam_matrix\":gamma_matrix},\n",
    "                                              return_df=True)\n",
    " 157:\n",
    "sin_diff_library = dummy_tr_sin_diff.fit_transform(data_matrix_df.drop([\"time\"], axis=1))\n",
    "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
    "\n",
    "# cop_ind\n",
    " 158: sin_diff_library\n",
    " 159: candidate_lib = pd.concat([data_matrix_df.drop(\"time\", axis=1), sin_diff_library], axis=1)\n",
    " 160: candidate_lib\n",
    " 161:\n",
    "import sympy\n",
    "\n",
    "from dae_finder import get_refined_lib, remove_paranth_from_feat\n",
    "\n",
    "# Adding the state variables as scipy symbols\n",
    "feat_list = list(data_matrix_df.columns)\n",
    "feat_list_str = \", \".join(remove_paranth_from_feat(data_matrix_df.columns))\n",
    "exec(feat_list_str+ \"= sympy.symbols(\"+str(feat_list)+\")\")\n",
    " 162:\n",
    "from dae_finder import sequentialThLin, AlgModelFinder\n",
    "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
    "                                       alpha=0.3,\n",
    "                                       fit_intercept=True)\n",
    " 163:\n",
    "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
    "                          features_to_fit=[\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
    "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"])\n",
    " 164: algebraic_model_lasso.best_models()\n",
    " 165: algebraic_model_lasso.best_models()[\"P_1\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.01]\n",
    " 166:\n",
    "feat = \"P_5\"\n",
    "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
    " 167:\n",
    "from dae_finder import sequentialThLin, AlgModelFinder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg_model = LinearRegression\n",
    "lin_reg_model_arg = {\"fit_intercept\": False}\n",
    "seq_th_model = sequentialThLin(custom_model=True,\n",
    "                               custom_model_ob = lin_reg_model,\n",
    "                               custom_model_arg= lin_reg_model_arg,\n",
    "                              coef_threshold=0.5)\n",
    "# seq_th_model = sequentialThLin(coef_threshold=0.1, fit_intercept=True)\n",
    "\n",
    "algebraic_model_th = AlgModelFinder(custom_model=True, custom_model_ob= seq_th_model)\n",
    " 168:\n",
    "algebraic_model_th.fit(candidate_lib, scale_columns= False,\n",
    "                          features_to_fit=[\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
    "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"])\n",
    " 169:\n",
    "#Best 10 models using R2 metrix\n",
    "algebraic_model_th.best_models()\n",
    " 170: algebraic_model_th.get_fitted_intercepts()\n",
    " 171:\n",
    "feat = \"P_2\"\n",
    "algebraic_model_th.best_models()[feat][abs(algebraic_model_th.best_models()[feat])>0.1]\n",
    " 172:\n",
    "from dae_finder import sequentialThLin, AlgModelFinder\n",
    "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
    "                                       alpha=0.3,\n",
    "                                       fit_intercept=True)\n",
    " 173:\n",
    "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
    "                          features_to_fit=[\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
    "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"])\n",
    " 174: algebraic_model_lasso.best_models()\n",
    " 175: algebraic_model_lasso.best_models()[\"P_1\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.01]\n",
    " 176:\n",
    "feat = \"P_5\"\n",
    "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
    " 177: from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    " 178:\n",
    "lin_model = LinearRegression()\n",
    "lass_model = Lasso(alpha=0.01)\n",
    "ridge_model = Ridge()\n",
    " 179:\n",
    "lin_model.fit(candidate_lib[['sin( Phi_3-Phi_0 -gamma_3,0 )', 'sin( Phi_3-Phi_1 -gamma_3,1 )',\n",
    "       'sin( Phi_3-Phi_2 -gamma_3,2 )', 'sin( Phi_3-Phi_3 -gamma_3,3 )',\n",
    "       'sin( Phi_3-Phi_4 -gamma_3,4 )', 'sin( Phi_3-Phi_5 -gamma_3,5 )']], candidate_lib[\"P_3\"])\n",
    " 180: gamma_df\n",
    " 181: %history -g\n",
    "\n",
    "\n",
    "Click to add a cell."
   ],
   "id": "67dbf3e144ac0338",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
